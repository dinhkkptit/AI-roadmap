D∆∞·ªõi ƒë√¢y l√† **Ki·∫øn th·ª©c chi ti·∫øt Tu·∫ßn 6 ‚Äì Supervised Fine-Tuning (SFT)** trong Roadmap h·ªçc LLM & GenAI.  
ƒê√¢y l√† m·ªôt trong nh·ªØng ph·∫ßn quan tr·ªçng nh·∫•t n·∫øu b·∫°n mu·ªën **tinh ch·ªânh LLM ƒë·ªÉ ph·ª•c v·ª• ·ª©ng d·ª•ng th·ª±c t·∫ø** (chatbot, RAG, h∆∞·ªõng d·∫´n, task-specific model).

---

# üü¶ **TU·∫¶N 6 ‚Äî Supervised Fine-Tuning (SFT)**

Tu·∫ßn n√†y b·∫°n h·ªçc:

1. SFT l√† g√¨?
    
2. D·ªØ li·ªáu SFT (instruction format)
    
3. Multi-turn conversation
    
4. C√°ch chu·∫©n h√≥a d·ªØ li·ªáu
    
5. Tokenization trong training
    
6. Hyperparameters quan tr·ªçng
    
7. Ki·∫øn tr√∫c training LLM (Trainer / PEFT / DeepSpeed)
    
8. Evaluate model sau SFT
    
9. Th·ª±c h√†nh pipeline SFT
    

---

# üß† **1. SFT l√† g√¨?**

**SFT (Supervised Fine-Tuning)** = hu·∫•n luy·ªán LLM b·∫±ng d·ªØ li·ªáu c√≥ **ƒë·∫ßu v√†o + ƒë·∫ßu ra chu·∫©n** (supervised).

Gi·ªëng nh∆∞:

`instruction ‚Üí input ‚Üí expected answer`

SFT gi√∫p m√¥ h√¨nh:

- l√†m theo h∆∞·ªõng d·∫´n (instruction-following)
    
- tr·∫£ l·ªùi l·ªãch s·ª±, c√≥ c·∫•u tr√∫c
    
- ph√π h·ª£p v·ªõi task domain
    
- bi·∫øt c√°ch format output
    
- gi·∫£m hallucination
    

Trong OpenAI, Anthropic, Meta ‚Üí **SFT l√† b∆∞·ªõc 1 trong RLHF pipeline**.

---

# üìÑ **2. D·ªØ li·ªáu SFT: Instruction Format**

Chu·∫©n ph·ªï bi·∫øn nh·∫•t hi·ªán nay:

`[   {     "instruction": "D·ªãch c√¢u sau sang ti·∫øng Anh",     "input": "Xin ch√†o",     "output": "Hello"   },   {     "instruction": "Gi·∫£i th√≠ch m√¥ h√¨nh LoRA",     "output": "LoRA l√†..."   } ]`

C√°c d·∫°ng format kh√°c:

- ChatML (used in Qwen)
    
- Alpaca-style
    
- LLaMA 3 chat format
    
- ShareGPT multi-turn format
    

---

# üí¨ **3. Multi-Turn Conversation (chat)**

LLM ng√†y nay d√πng ki·∫øn tr√∫c chat ‚Üí c·∫ßn d·ªØ li·ªáu nhi·ªÅu l∆∞·ª£t:

`[   {"role": "user", "content": "xin ch√†o"},   {"role": "assistant", "content": "ch√†o b·∫°n"},   {"role": "user", "content": "h√¥m nay tr·ªùi th·∫ø n√†o?"},   {"role": "assistant", "content": "tr·ªùi ƒë·∫πp"} ]`

SFT chat gi√∫p LLM:

- nh·ªõ b·ªëi c·∫£nh
    
- gi·ªØ phong c√°ch tr√≤ chuy·ªán
    
- kh√¥ng qu√™n l·ªãch s·ª≠ h·ªôi tho·∫°i
    

---

# üßπ **4. Chu·∫©n h√≥a d·ªØ li·ªáu SFT**

C√°c b∆∞·ªõc th·ª±c t·∫ø:

### ‚úî 4.1. Lo·∫°i b·ªè d·ªØ li·ªáu b·∫©n

- c√¢u v√¥ nghƒ©a
    
- c√¢u d·ªãch kh√¥ng chu·∫©n
    
- th√¥ng tin sai l·ªách
    

### ‚úî 4.2. C√¢n b·∫±ng d·ªØ li·ªáu

- kh√¥ng ƒë·ªÉ 1 t√°c v·ª• chi·∫øm 90% dataset
    

### ‚úî 4.3. Chu·∫©n h√≥a ng√¥n ng·ªØ

- vi·∫øt ƒë√∫ng ch√≠nh t·∫£
    
- format consistent
    

### ‚úî 4.4. Gi·ªõi h·∫°n ƒë·ªô d√†i

- t√°ch c√¢u d√†i > 4096 tokens
    

---

# üî§ **5. Tokenization trong training**

Ph·∫£i decode d·ªØ li·ªáu theo ƒë√∫ng tokenizer c·ªßa model:

- LLaMA tokenizer
    
- Qwen tokenizer
    
- Mistral tokenizer
    
- Tiktoken for GPT-like
    

### C·∫£nh b√°o:

‚ùå Kh√¥ng token h√≥a b·∫±ng tokenizer kh√°c model  
‚ùå Multi-lingual ‚Üí ph·∫£i ki·ªÉm tra token splitting

---

# üßÆ **6. Hyperparameters quan tr·ªçng**

|Tham s·ªë|G·ª£i √Ω (SFT 7B‚Äì13B)|
|---|---|
|learning_rate|2e-5 ‚Üí 5e-5|
|batch_size|1‚Äì8 (LoRA/QLoRA)|
|max_seq_len|1024‚Äì4096|
|epochs|1‚Äì3|
|warmup_steps|50‚Äì200|
|weight_decay|0.0|
|gradient_accumulation|4‚Äì32|
|LoRA rank|8‚Äì32|

‚úî V·ªõi QLoRA:

`learning_rate = 2e-4 lora_alpha = 16‚Äì64`

---

# üèóÔ∏è **7. Training pipeline**

### B·ªô khung chu·∫©n:

1. Tokenize dataset
    
2. Format dataset theo ChatML / Alpaca
    
3. Load model (FP16 cho LoRA, NF4 cho QLoRA)
    
4. Ch√®n LoRA adapters
    
5. Setup Trainer (HuggingFace)
    
6. Training + evaluation
    
7. Merge LoRA (t√πy ch·ªçn)
    
8. Export model
    

### C√¥ng c·ª• ph·ªï bi·∫øn:

- **LlamaFactory** (UI/CLI, d·ªÖ nh·∫•t)
    
- **Axolotl** (d√πng ph·ªï bi·∫øn trong c√¥ng ty)
    
- **PEFT + Transformers** (t·ª± code, linh ho·∫°t nh·∫•t)
    
- **DeepSpeed / Accelerate** (t·ªëi ∆∞u GPU l·ªõn)
    

---

# üìä **8. Evaluate model sau SFT**

### C√°ch test:

- test v·ªõi prompt t·ª´ dataset
    
- test ngo√†i dataset ƒë·ªÉ generalize
    
- d√πng NLP metrics: BLEU, ROUGE
    
- d√πng ƒë√°nh gi√° LLM: GPT-4 Judge
    
- test hallucination
    
- test long context
    

### RAG-specific SFT:

- ƒë·∫£m b·∫£o model d√πng context t·ªët
    
- kh√¥ng tr·∫£ l·ªùi sai n·∫øu kh√¥ng c√≥ th√¥ng tin
    

---

# üß™ **9. V√≠ d·ª• th·ª±c t·∫ø: SFT Qwen2 7B b·∫±ng QLoRA**

### ƒêo·∫°n code m·∫´u (Python + Transformers):

`from transformers import AutoModelForCausalLM, AutoTokenizer from peft import LoraConfig, get_peft_model from datasets import load_dataset  model_name = "Qwen/Qwen2-7B"  tokenizer = AutoTokenizer.from_pretrained(model_name) model = AutoModelForCausalLM.from_pretrained(     model_name,     load_in_4bit=True,     device_map="auto" )  lora = LoraConfig(     r=16,     lora_alpha=32,     target_modules=["q_proj","v_proj"],     lora_dropout=0.05 )  model = get_peft_model(model, lora) dataset = load_dataset("json", data_files="sft.json")  # training arguments ...`

ƒê√¢y l√† m·ªôt pipeline ƒë·∫ßy ƒë·ªß gi√∫p b·∫°n fine-tune th·ª±c s·ª±.

---

# üß† **10. Best Practices (r·∫•t quan tr·ªçng)**

‚úî Kh√¥ng d√πng learning rate qu√° cao  
‚úî Kh√¥ng train qu√° l√¢u ‚Üí model overfit, tr·∫£ l·ªùi k·ª≥ qu·∫∑c  
‚úî Lu√¥n test model tr√™n prompt ch∆∞a th·∫•y trong dataset  
‚úî Format r√µ r√†ng:

`<|user|>  <|assistant|>`

‚úî Ch·ªâ m·ªü LoRA ·ªü c√°c layer attention ‚Üí quality ·ªïn ƒë·ªãnh  
‚úî D·ªØ li·ªáu ph·∫£i s·∫°ch v√† ph√π h·ª£p task

---

# üéÅ **T√≥m t·∫Øt Tu·∫ßn 6**

|M·ª•c|Hi·ªÉu g√¨?|
|---|---|
|SFT|Fine-tune theo d·ªØ li·ªáu gi√°m s√°t|
|D·ªØ li·ªáu SFT|Instruction, input, output|
|Conversation|Multi-turn chat|
|Tokenization|R·∫•t quan tr·ªçng|
|Hyperparams|LR, batch, seq_len|
|Training tools|LlamaFactory, Axolotl, PEFT|
|Evaluate|Test ch·∫•t l∆∞·ª£ng sau training|

---

# ‚ùì B·∫°n mu·ªën ti·∫øp g√¨ ti·∫øp theo?

- Xu·∫•t **Tu·∫ßn 6** th√†nh file `.md`
    
- L√†m v√≠ d·ª• dataset ho√†n ch·ªânh
    
- T·∫°o code full training (Axolotl / PEFT / LlamaFactory)
    
- T∆∞ v·∫•n hyperparameters cho GPU c·ªßa b·∫°n
    
- T·∫°o lesson quiz b√†i ki·ªÉm tra tu·∫ßn 6
    

Ch·ªâ c·∫ßn n√≥i nh√©!
